---
title: "Appendix"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(rpart.plot)
library(readxl)
```

```{r}
data = read_csv("data.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::select(-id) %>%
  mutate(
    class = case_when(
      class == 2 ~ 'B',
      class == 4 ~ 'M'
    ) %>% as.factor(),
    bare_nucleoli = na_if(bare_nucleoli, '?'),
    bare_nucleoli = as.integer(bare_nucleoli)
  ) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), median(., na.rm = T)))
```

```{r}
set.seed(2020)
rowTrain <- createDataPartition(y = data$class,
                                p = 2/3,
                                list = FALSE)
```

Linear methods
*Logistic regression*
```{r}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(2020)
glm.fit <- train(class~., data, 
                 subset = rowTrain,
                 method = "glm",
                 metric = "ROC",
                 trControl = ctrl)
```

*Regularized logistic regression*
```{r}
glmnGrid <- expand.grid(alpha = seq(0, 1, length = 6),
                        lambda = exp(seq(-8, -2, length = 20)))
set.seed(2020)
glmn.fit <- train(class~., data, 
                    subset = rowTrain,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)
glmn_plot = plot(glmn.fit, xTrans = function(x) log(x), 
                 highlight = T)   
glmn.fit$bestTune
```

Discriminant analysis
*LDA*
```{r}
set.seed(2020)
lda.fit <- train(class~., data, 
                   subset = rowTrain,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
```

*QDA*
```{r}
set.seed(2020)
qda.fit <- train(class~., data, 
                   subset = rowTrain,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)
```

*Naive Bayes (NB)*
```{r}
set.seed(2020)
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(.2, 3, by = .2))
nb.fit <- train(class~., data, 
                  subset = rowTrain,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)
nb_plot = plot(nb.fit, xTrans = function(x) log(x), 
                 highlight = T)   
```

KNN
```{r, warning=FALSE}
set.seed(2020)
knn.fit <- train(class~., data, 
                   subset = rowTrain,
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl)
knn_plot = ggplot(knn.fit, highlight = T)
```

Tree-based methods
#### Cart
```{r}
set.seed(2020)
rpart.fit <- train(class~., data, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 20))),
                   trControl = ctrl,
                   metric = "ROC")
cart_plot = ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

#### CIT
```{r}
set.seed(2020)
ctree.fit <- train(class~., data, 
                   subset = rowTrain,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-2, -1, length = 20))),
                   metric = "ROC",
                   trControl = ctrl)
cit_plot = ggplot(ctree.fit, highlight = TRUE)
plot(ctree.fit$finalModel)
```

Ensemble methods
*Random forests*
```{r}
rf.grid <- expand.grid(mtry = 1:9,
                       splitrule = "gini",
                       min.node.size = 1:10)
set.seed(2020)
rf.fit <- train(class~., data,
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
rf_plot = ggplot(rf.fit, highlight = TRUE)
```

*Boosting*
#### Binomial loss
```{r}
gbmB.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:10,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(2020)
# Binomial loss function
gbmB.fit <- train(class~., data,
                 subset = rowTrain, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)
gbmB_plot = ggplot(gbmB.fit, highlight = TRUE)
```

#### AdaBoost
```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:10,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(2020)
# Adaboost loss function
gbmA.fit <- train(class~., data,
                 subset = rowTrain, 
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
gbmA_plot = ggplot(gbmA.fit, highlight = TRUE)
```

Support vector machines
*Linear boundary*
```{r}
set.seed(2020)
svml.fit <- train(class~., data,
                  subset = rowTrain, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-5,0,len=20))),
                  trControl = ctrl)
svml_plot = ggplot(svml.fit, highlight = TRUE)
```

*Radial kernal*
```{r}
svmr.grid <- expand.grid(C = exp(seq(-1,4,len=10)),
                         sigma = exp(seq(-6,-2,len=10)))
set.seed(2020)             
svmr.fit <- train(class~., data, 
                  subset = rowTrain,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
svmr_plot = ggplot(svmr.fit, highlight = TRUE)
```

Model Selection
```{r}
resamp <- resamples(list(GLM = glm.fit, 
                         GLMNET = glmn.fit,
                         LDA = lda.fit, 
                         QDA = qda.fit,
                         NB = nb.fit, 
                         KNN = knn.fit,
                         ctree = ctree.fit, 
                         rpart = rpart.fit,
                         rf = rf.fit,
                         gbmA = gbmA.fit,
                         gbmB = gbmB.fit,
                         svmr = svmr.fit, 
                         svml = svml.fit))
summary(resamp)   # training error rate for models
bwplot(resamp, metric = "ROC")
```

