---
title: "Appendix"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(rpart.plot)
library(gridExtra)
library(readxl)
```

```{r}
data = read_csv("data.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::select(-id) %>%
  mutate(
    class = case_when(
      class == 2 ~ 'B',
      class == 4 ~ 'M'
    ) %>% as.factor(),
    bare_nucleoli = na_if(bare_nucleoli, '?'),
    bare_nucleoli = as.integer(bare_nucleoli)
  ) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), median(., na.rm = T)))
```

```{r}
set.seed(2020)
rowTrain <- createDataPartition(y = data$class,
                                p = 2/3,
                                list = FALSE)
```

Linear methods
*Logistic regression*
```{r}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(2020)

glm.fit = train(class~., data, 
                subset = rowTrain,
                 method = "glm",
                 metric = "ROC",
                 trControl = ctrl)

var_glm = varImp(glm.fit)$importance
```

*Regularized logistic regression*
```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-10, -1, length = 30)))

set.seed(2020)
glmn.fit <- train(class~., data, 
                    subset = rowTrain,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)
glmn_plot = plot(glmn.fit, xTrans = function(x) log(x), 
                 highlight = T)   
glmn.fit$bestTune

var_glmn = varImp(glmn.fit)$importance
```

Discriminant analysis
*LDA*
```{r}
set.seed(2020)
lda.fit <- train(class~., as.data.frame(data), 
                   subset = rowTrain,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)

# need to convert data in lda to data frame, otherwise varImp doesnt work
var_lda = varImp(lda.fit)$importance
```

*QDA*
```{r}
set.seed(2020)
qda.fit <- train(class~., as.data.frame(data), 
                   subset = rowTrain,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

var_qda = varImp(qda.fit)$importance
```

*Naive Bayes (NB)*
```{r}
set.seed(2020)
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(0.8, 10, by = .4))
nb.fit <- train(class~., as.data.frame(data), 
                  subset = rowTrain,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)
nb_plot = plot(nb.fit, xTrans = function(x) log(x), 
                 highlight = T) 

var_nb = varImp(nb.fit)$importance
```

KNN
```{r, warning=FALSE}
set.seed(2020)
knn.fit <- train(class~., as.data.frame(data), 
                   subset = rowTrain,
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl)
knn_plot = ggplot(knn.fit, highlight = T)

var_knn = varImp(knn.fit)$importance
```

Tree-based methods
#### Cart
```{r}
set.seed(2020)
rpart.fit <- train(class~., data, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 20))),
                   trControl = ctrl,
                   metric = "ROC")
cart_plot = ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)

var_rpart = varImp(rpart.fit)$importance
```

#### CIT
```{r}
set.seed(2020)
ctree.fit <- train(class~., as.data.frame(data), 
                   subset = rowTrain,
                   method = "ctree",
                   tuneGrid = data.frame(
                     mincriterion = 1 - exp(seq(-2, -1, length = 20))
                   ), 
                   metric = "ROC",
                   trControl = ctrl)

cit_plot = ggplot(ctree.fit, highlight = TRUE)
plot(ctree.fit$finalModel)

var_ctree = varImp(ctree.fit)$importance
```

Ensemble methods
*Random forests*
```{r}
rf.grid <- expand.grid(mtry = 1:9,
                       splitrule = "gini",
                       min.node.size = 1:9)
set.seed(2020)
rf.fit <- train(class~., as.data.frame(data),
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
rf_plot = ggplot(rf.fit, highlight = TRUE)

# varImp(rf.fit)
```

*Boosting*
#### Binomial loss
```{r}
gbmB.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:9,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(2020)
# Binomial loss function
gbmB.fit <- train(class~., data,
                 subset = rowTrain, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)
gbmB_plot = ggplot(gbmB.fit, highlight = TRUE)

# varImp(gbmB.fit)
```

#### AdaBoost
```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:9,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(2020)
# Adaboost loss function
gbmA.fit <- train(class~., data,
                 subset = rowTrain, 
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
gbmA_plot = ggplot(gbmA.fit, highlight = TRUE)

# varImp(gbmA.fit)
```

Support vector machines
*Linear boundary*
```{r}
set.seed(2020)
svml.fit <- train(class~., as.data.frame(data),
                  subset = rowTrain, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-6,-2,len=20))),
                  trControl = ctrl)
svml_plot = ggplot(svml.fit, highlight = TRUE)

var_svml = varImp(svml.fit)$importance
```

*Radial kernal*
```{r}
svmr.grid <- expand.grid(C = exp(seq(-1,4,len=10)),
                         sigma = exp(seq(-7,-3,len=20)))
set.seed(2020)             
svmr.fit <- train(class~., as.data.frame(data), 
                  subset = rowTrain,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
svmr_plot = ggplot(svmr.fit, highlight = TRUE)

var_svmr = varImp(svmr.fit)$importance
```

Visualization

```{r}
grid.arrange(glmn_plot, nb_plot, knn_plot,
             cart_plot, cit_plot, rf_plot,
             gbmB_plot, gbmA_plot, svml_plot, svmr_plot)
```


Model Selection
```{r}
resamp <- resamples(list(GLM = glm.fit, 
                         GLMNET = glmn.fit,
                         LDA = lda.fit, 
                         QDA = qda.fit,
                         NB = nb.fit, 
                         KNN = knn.fit,
                         ctree = ctree.fit, 
                         rpart = rpart.fit,
                         rf = rf.fit,
                         gbmA = gbmA.fit,
                         gbmB = gbmB.fit,
                         svmr = svmr.fit, 
                         svml = svml.fit))
summary(resamp)   # training error rate for models
bwplot(resamp, metric = "ROC")
```

Test data performance
```{r}
glm.pred <- predict(glm.fit, newdata = data[-rowTrain,], type = "prob")[,1]
glmn.pred <- predict(glmn.fit, newdata = data[-rowTrain,], type = "prob")[,1]
nb.pred <- predict(nb.fit, newdata = data[-rowTrain,], type = "prob")[,1]
lda.pred <- predict(lda.fit, newdata = data[-rowTrain,], type = "prob")[,1]
qda.pred <- predict(qda.fit, newdata = data[-rowTrain,], type = "prob")[,1]
knn.pred <- predict(knn.fit, newdata = data[-rowTrain,], type = "prob")[,1]
rpart.pred <- predict(rpart.fit, newdata = data[-rowTrain,], type = "prob")[,1]
ctree.pred <- predict(ctree.fit, newdata = data[-rowTrain,], type = "prob")[,1]
rf.pred <- predict(rf.fit, newdata = data[-rowTrain,], type = "prob")[,1]
gbmB.pred <- predict(gbmB.fit, newdata = data[-rowTrain,], type = "prob")[,1]
gbmA.pred <- predict(gbmA.fit, newdata = data[-rowTrain,], type = "prob")[,1]
pred.svml <- predict(svml.fit, newdata = data[-rowTrain,], type = "prob")[,1]
pred.svmr <- predict(svmr.fit, newdata = data[-rowTrain,], type = "prob")[,1]

roc.glm <- roc(data$class[-rowTrain], glm.pred)
roc.glmn <- roc(data$class[-rowTrain], glmn.pred)
roc.nb <- roc(data$class[-rowTrain], nb.pred)
roc.lda <- roc(data$class[-rowTrain], lda.pred)
roc.qda <- roc(data$class[-rowTrain], qda.pred)
roc.knn <- roc(data$class[-rowTrain], knn.pred)
roc.rpart <- roc(data$class[-rowTrain], rpart.pred)
roc.ctree <- roc(data$class[-rowTrain], ctree.pred)
roc.rf <- roc(data$class[-rowTrain], rf.pred)
roc.gbmA <- roc(data$class[-rowTrain], gbmA.pred)
roc.gbmB <- roc(data$class[-rowTrain], gbmB.pred)
roc.svml <- roc(data$class[-rowTrain], svml.pred)
roc.svmr <- roc(data$class[-rowTrain], svmr.pred)

plot(roc.glm, legacy.axes = TRUE)
plot(roc.glmn, col = 2, add = TRUE)
plot(roc.lda, col = 3, add = TRUE)
plot(roc.qda, col = 4, add = TRUE)
plot(roc.nb, col = 5, add = TRUE)
plot(roc.knn, col = 6, add = TRUE)
plot(roc.rpart, col = 7, add = TRUE)
plot(roc.ctree, col = 8, add = TRUE)
plot(roc.rf, col = 9, add = TRUE)
plot(roc.gbmA, col = 10, add = TRUE)
plot(roc.gbmB, col = 11, add = TRUE)
plot(roc.svml, col = 12, add = TRUE)
plot(roc.svmr, col = 13, add = TRUE)

auc <- c(roc.glm$auc[1], roc.glmn$auc[1], roc.lda$auc[1],
         roc.qda$auc[1], roc.nb$auc[1], roc.knn$auc[1],
         roc.rpart$auc[1], roc.ctree$auc[1], roc.rf$auc[1], 
         roc.gbmA$auc[1], roc.gbmB$auc[1],
         roc.svml$auc[1], roc.svmr$auc[1])

modelNames <- c("glm","glmn","lda","qda","nb","knn","rpart","ctree","rf","gbmA","gbmB","svml","svmr")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:13, lwd = 2)
```

variable importance table
```{r}
clean_var_table = function(table) {
  # vc: variable columns
  if (ncol(table) == 1) {
    table = table %>% 
      rownames_to_column() %>% 
      janitor::clean_names() %>% 
      rename('Importance' = `overall`, 
             'Variable' = `rowname`)
    
    return(table)
    
  } else if (ncol(table) == 2) {
    table = table %>% 
      rownames_to_column() %>% 
      janitor::clean_names() %>% 
      select(rowname, `b`) %>% 
      rename('Importance' = `b`, 
             'Variable' = `rowname`)
    
    return(table)
      
  }
}

tt_ctree = clean_var_table(table = var_ctree) %>% 
  rename(ctree = `Importance`)
tt_glm = clean_var_table(table = var_glm) %>% 
  rename(glm = `Importance`)
tt_glmn = clean_var_table(table = var_glmn) %>% 
  rename(glmn = `Importance`)
tt_knn = clean_var_table(table = var_knn) %>% 
  rename(knn = `Importance`)
tt_lda = clean_var_table(table = var_lda) %>% 
  rename(lda = `Importance`)
tt_nb = clean_var_table(table = var_nb) %>% 
  rename(nb = `Importance`)
tt_qda = clean_var_table(table = var_qda) %>% 
  rename(qda = `Importance`)
tt_rpart = clean_var_table(table = var_rpart) %>% 
  rename(rpart = `Importance`)
tt_svml = clean_var_table(table = var_svml) %>% 
  rename(svml = `Importance`)
tt_svmr = clean_var_table(table = var_svmr) %>% 
  rename(svmr = `Importance`)

tt_final = left_join(tt_glm, tt_glmn, by = "Variable") %>% 
  left_join(., tt_lda, by = "Variable") %>% 
  left_join(., tt_qda, by = "Variable") %>% 
  left_join(., tt_nb, by = "Variable") %>% 
  left_join(., tt_knn, by = "Variable") %>% 
  left_join(., tt_rpart, by = "Variable") %>% 
  left_join(., tt_ctree, by = "Variable")%>% 
  left_join(., tt_svml, by = "Variable")%>% 
  left_join(., tt_svmr, by = "Variable")
```

