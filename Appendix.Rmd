---
title: "Appendix"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(rpart.plot)
```

```{r}
data=read.csv("data.csv")[,-33]
table(is.na(data)) # no missing value
```

```{r}
set.seed(2020)
rowTrain <- createDataPartition(y = data$diagnosis,
                                p = 2/3,
                                list = FALSE)
```

Classification Trees
*Cart*
```{r}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(2020)
rpart.fit <- train(diagnosis~., data, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 20))),
                   trControl = ctrl,
                   metric = "ROC")
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

*CIT*
```{r}
set.seed(2020)
ctree.fit <- train(diagnosis~., data, 
                   subset = rowTrain,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-2, -1, length = 20))),
                   metric = "ROC",
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
plot(ctree.fit$finalModel)
```

Ensemble methods
*Random forests*
```{r}
rf.grid <- expand.grid(mtry = 1:10,
                       splitrule = "gini",
                       min.node.size = 1:10)
set.seed(2020)
rf.fit <- train(diagnosis~., data,
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
ggplot(rf.fit, highlight = TRUE)
```

*Boosting*
#### Binomial loss
```{r}
gbmB.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:10,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(2020)
# Binomial loss function
gbmB.fit <- train(diagnosis~., data,
                 subset = rowTrain, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)
ggplot(gbmB.fit, highlight = TRUE)
```

#### AdaBoost
```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:10,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(2020)
# Adaboost loss function
gbmA.fit <- train(diagnosis~., data,
                 subset = rowTrain, 
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
ggplot(gbmA.fit, highlight = TRUE)
```

Support vector machines
*Linear boundary*
```{r}
set.seed(2020)
svml.fit <- train(diagnosis~., data,
                  subset = rowTrain, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-5,0,len=20))),
                  trControl = ctrl)
ggplot(svml.fit, highlight = TRUE)
```

*Radial kernal*
```{r}
svmr.grid <- expand.grid(C = exp(seq(-1,4,len=10)),
                         sigma = exp(seq(-6,-2,len=10)))
set.seed(2020)             
svmr.fit <- train(diagnosis~., data, 
                  subset = rowTrain,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
ggplot(svmr.fit, highlight = TRUE)
```

Model Selection
```{r}
rpart.pred <- predict(rpart.fit, newdata = data[-rowTrain,], type = "prob")[,1]
ctree.pred <- predict(ctree.fit, newdata = data[-rowTrain,], type = "prob")[,1]
rf.pred <- predict(rf.fit, newdata = data[-rowTrain,], type = "prob")[,1]
gbmB.pred <- predict(gbmB.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
gbmA.pred <- predict(gbmA.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
svml.pred <- predict(svml.fit, newdata = dat[-rowTrain,])
svmr.pred <- predict(svmr.fit, newdata = dat[-rowTrain,])
resamp <- resamples(list(ctree = ctree.fit, 
                         rpart = rpart.fit,
                         rf = rf.fit,
                         gbmA = gbmA.fit,
                         gbmB = gbmB.fit,
                         svmr = svmr.fit, 
                         svml = svml.fit))
summary(resamp)   # training error rate for 2 models
```

